# Direct Preference Optimization (DPO) Implementation on GPT-2

This project is a PyTorch implementation of the **Direct Preference Optimization (DPO)** algorithm, a state-of-the-art technique for fine-tuning Large Language Models (LLMs) with human preferences. The base model used is `gpt2`, and it is fine-tuned on the "Helpful and Harmless" (HH-RLHF) dataset. The goal is to benchmark its performance against a standard Supervised Fine-Tuning (SFT) baseline.

This work is a practical application of the concepts from the Stanford research paper: **"Direct Preference Optimization: Your Language Model is Secretly a Reward Model"**.

---

## 1. Context: DPO vs. Traditional RLHF

Aligning LLMs with human preferences is a crucial step to ensure their safety and utility. The standard method, **Reinforcement Learning from Human Feedback (RLHF)**, is a complex, multi-stage process:

1.  **Supervised Fine-Tuning (SFT):** Fine-tuning the base LLM on a high-quality dataset.
2.  **Reward Modeling:** Training a separate reward model to learn human preferences from a dataset of ranked responses.
3.  **RL Fine-Tuning:** Further fine-tuning the SFT model using a reinforcement learning algorithm (like PPO) guided by the reward model.

**Direct Preference Optimization (DPO)** offers a simpler and more stable alternative. It eliminates the need for a separate reward model and a complex RL loop. Instead, DPO uses preference data directly to adjust the LLM's policy, treating it implicitly as its own reward model.

**Advantages of DPO:**
* **Simplicity:** No reward model to train, no complex sampling required.
* **Stability:** Easier to train and less sensitive to hyperparameters than PPO.
* **Efficiency:** Less computationally and memory-intensive.

---

## 2. Project Pipeline

This project implements a complete pipeline to compare the effectiveness of DPO.

* **Models:** Three models are trained and compared:
    1.  **Reference Model (`pi_ref`):** The original, unmodified pre-trained `gpt2` model.
    2.  **SFT Model (Supervised Fine-Tuning):** `gpt2` fine-tuned only on the "chosen" (preferred) responses from the dataset.
    3.  **DPO Model (`pi_theta`):** `gpt2` fine-tuned using the DPO loss function, which learns directly from preference pairs.
    *For the SFT and DPO models, the **first 8 layers of the Transformer are frozen** to preserve the base model's knowledge while adapting its higher-level representations.*

* **Dataset:** `Anthropic/hh-rlhf`, which contains triplets of `(prompt, chosen_response, rejected_response)`.

* **Training:** The training loop is implemented in PyTorch and features a `RMSprop` optimizer, a linear warm-up scheduler, and gradient clipping for stability.

* **Evaluation:** Performance is measured along two axes, as in the original DPO paper:
    * **Implicit Reward:** Measures how well the model aligns with the preference data.
    * **KL Divergence:** Measures how much the model's policy has deviated from the reference model. A lower KL divergence is generally better, as it indicates the model has not forgotten its original capabilities.

---

## 3. Results and Analysis

The script generates several key analyses to evaluate the performance of the DPO algorithm.

### DPO Loss Evolution

The plot below shows the progression of the DPO loss over the training epochs. A decreasing loss indicates that the model is successfully learning to align with the human preferences.
![DPO Loss Evolution](./images/dpo_loss_evolution.png)

### Model Comparison (DPO vs. SFT vs. Reference)

This plot positions the three models on a Reward vs. KL Divergence axis. The DPO model achieves a better trade-off, securing a higher reward than the SFT model for a similar level of KL divergence, demonstrating its superior alignment capability.
![Reward vs KL Trade-off](./images/figure2_model_comparison.png)

### Sensitivity Analysis of the β Parameter

The `beta` (β) parameter in the DPO loss function controls the strength of the KL constraint. An analysis was conducted for different values of β. The plot shows that a higher β pushes the model to seek higher rewards, often at the cost of greater divergence from the original model. A β of 0.5 appears to offer a good balance.
![Beta Sensitivity Analysis](./images/figure4_beta_sensitivity.png)

### Qualitative Analysis of Generations

Below are sample responses generated by the three models to qualitatively assess their behavior.

| Prompt                                | Reference Model (GPT-2) | SFT Model                     | DPO Fine-Tuned Model               |
| :------------------------------------ | :-------------------------- | :----------------------------- | :----------------------------------- |
| "What are some fun things to do in Paris?" | *(Base model response...)* | *(Response fine-tuned on positive examples...)* | *(Response aligned via DPO...)* |
| "How can I learn to code for free?" | *(...)* | *(...)* | *(...)* |
| "What is the meaning of life?"        | *(...)* | *(...)* | *(...)* |

*(Fill this table with the actual output from your script to showcase the improvement.)*

---

## 4. How to Run

### Prerequisites
* Python 3.8+
* PyTorch & CUDA-enabled GPU (e.g., T4 on Google Colab)
* `transformers`, `datasets`, `tqdm`, `matplotlib`

### Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/YOUR_USERNAME/YOUR_REPO.git](https://github.com/YOUR_USERNAME/YOUR_REPO.git)
    cd YOUR_REPO
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

### Running the Script

The main script executes the entire pipeline. Ensure you are in an environment with a GPU available.

```bash
python dpo_implementation.py
```

The script will:
1.  Download the dataset and the base model from Hugging Face.
2.  Train and evaluate the SFT baseline model.
3.  Train and evaluate the main DPO model.
4.  Run the sensitivity analysis for the `beta` parameter.
5.  Display the result plots and generation examples.
6.  Save the final DPO model to the `dpo_gpt2_model_final/` directory.

---

## 5. Reference

* **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**
    * *Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn.*
    * [Link to Paper (arXiv)](https://arxiv.org/abs/2305.18290)
